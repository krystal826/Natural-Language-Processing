{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Lab03-Task01-Tokenization.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krystal826/Natural-Language-Processing/blob/main/Lab03_Task01_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqcNLRj14vqd"
      },
      "source": [
        "# Lab03 Task 01 instructions: \n",
        "\n",
        "# Below are Python codes to explore word tokenization and sentence segmentation\n",
        "\n",
        "# 1. Run all codes, see the output issued and understand how word normalization is done using Python programming.\n",
        "\n",
        "# 2. If you have error about \"undefined name\", it means a particular package isn't donwloaded yet. Please execute the following code\n",
        "\n",
        "# download.nltk() - this may take quite a long time to execute, else\n",
        "# download.nltk(\"name-package\") - just download the particular package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiy9vCq_4vqh",
        "outputId": "d1da8a6c-465d-4b2b-db69-9a03472de60a"
      },
      "source": [
        "# import NLTK package\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "# Sample of sentences\n",
        "text = \"Python is a really powerful programming language!\"\n",
        "\n",
        "word_tokenize(text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Python', 'is', 'a', 'really', 'powerful', 'programming', 'language', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZf0eu904vqi"
      },
      "source": [
        "# Word Tokenization: NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlQhLwLg4vqi",
        "outputId": "6f5130c0-7370-4ab4-f76e-4f6622c452e7"
      },
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "# Sample of sentences\n",
        "text = \"We will discuss  briefly about the basic syntax, \" \\\n",
        "\"structure and design philosophies. There is a hierarchical syntax for \" \\\n",
        "\"Python code which you should remember when writing code! Python is a really powerful programming language!\"\n",
        "\n",
        "word_tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We',\n",
              " 'will',\n",
              " 'discuss',\n",
              " 'briefly',\n",
              " 'about',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'syntax',\n",
              " ',',\n",
              " 'structure',\n",
              " 'and',\n",
              " 'design',\n",
              " 'philosophies',\n",
              " '.',\n",
              " 'There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'hierarchical',\n",
              " 'syntax',\n",
              " 'for',\n",
              " 'Python',\n",
              " 'code',\n",
              " 'which',\n",
              " 'you',\n",
              " 'should',\n",
              " 'remember',\n",
              " 'when',\n",
              " 'writing',\n",
              " 'code',\n",
              " '!',\n",
              " 'Python',\n",
              " 'is',\n",
              " 'a',\n",
              " 'really',\n",
              " 'powerful',\n",
              " 'programming',\n",
              " 'language',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrjlY1N34vqi",
        "outputId": "94e07ba8-f7a5-42b5-a495-158d3803aa7b"
      },
      "source": [
        "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
        "word_tokenize(sentence)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'brown',\n",
              " 'fox',\n",
              " 'was',\n",
              " \"n't\",\n",
              " 'that',\n",
              " 'quick',\n",
              " 'and',\n",
              " 'he',\n",
              " 'could',\n",
              " \"n't\",\n",
              " 'win',\n",
              " 'the',\n",
              " 'race']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYsfmwR74vqj",
        "outputId": "4d4aaad8-a5d7-4dfa-d88b-1b3e0a03529d"
      },
      "source": [
        "#Treebank word tokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
        "TreebankWordTokenizer().tokenize(sentence)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'brown',\n",
              " 'fox',\n",
              " 'was',\n",
              " \"n't\",\n",
              " 'that',\n",
              " 'quick',\n",
              " 'and',\n",
              " 'he',\n",
              " 'could',\n",
              " \"n't\",\n",
              " 'win',\n",
              " 'the',\n",
              " 'race']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6UgvrjH4vqj"
      },
      "source": [
        "# Look at how word_tokenize and TreebankWordTokenizer token the word \"wasn't\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HK5hGUeH4vqk",
        "outputId": "ae8880ea-5494-46ce-80ea-85dc28bfe178"
      },
      "source": [
        "# Regular expression Tokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "token_pattern = RegexpTokenizer('\\w+')   #extract word\n",
        "\n",
        "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
        "token_pattern.tokenize(sentence)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'brown',\n",
              " 'fox',\n",
              " 'wasn',\n",
              " 't',\n",
              " 'that',\n",
              " 'quick',\n",
              " 'and',\n",
              " 'he',\n",
              " 'couldn',\n",
              " 't',\n",
              " 'win',\n",
              " 'the',\n",
              " 'race']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVaUcV5c4vqk"
      },
      "source": [
        "# What does the pattern \\w+ in RegexpTokenizer mean?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpkA7RRA4vql",
        "outputId": "40df1b32-974f-4bfe-bb53-116e58431e55"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "gap_pattern = RegexpTokenizer('\\s+', gaps=True)   #tokenie word \n",
        "\n",
        "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
        "gap_pattern.tokenize(sentence)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'brown',\n",
              " 'fox',\n",
              " \"wasn't\",\n",
              " 'that',\n",
              " 'quick',\n",
              " 'and',\n",
              " 'he',\n",
              " \"couldn't\",\n",
              " 'win',\n",
              " 'the',\n",
              " 'race']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tASquaV4vql",
        "outputId": "2b8ab03a-0087-466a-d692-bf6a35aa59d9"
      },
      "source": [
        "# Word Punctuation tokenizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
        "WordPunctTokenizer().tokenize(sentence)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'brown',\n",
              " 'fox',\n",
              " 'wasn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'that',\n",
              " 'quick',\n",
              " 'and',\n",
              " 'he',\n",
              " 'couldn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'win',\n",
              " 'the',\n",
              " 'race']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaESkd954vql",
        "outputId": "aae85caf-95a7-47dd-d405-a0aa0d231467"
      },
      "source": [
        "# Tokenize word based of whitespace\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
        "WordPunctTokenizer().tokenize(sentence)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'brown',\n",
              " 'fox',\n",
              " 'wasn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'that',\n",
              " 'quick',\n",
              " 'and',\n",
              " 'he',\n",
              " 'couldn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'win',\n",
              " 'the',\n",
              " 'race']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwQQ-gdo4vqm"
      },
      "source": [
        "# Sentence Segmentation: NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHn0b1eN4vqm",
        "outputId": "d3c610f8-87d9-4b26-88d8-6467422fb6ad"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# NLTK sent_tokenize\n",
        "default_st = nltk.sent_tokenize\n",
        "\n",
        "text = \"We will discuss briefly with Dr. John about the basic syntax, structure and design philosophies. There is a hierarchical syntax for Python code which you should remember when writing code! Python is a really powerful programming language! Why is Python so special for you?\"\n",
        "\n",
        "sent_segment = default_st(text)\n",
        "print(\"Total sentences in doc: \", len(sent_segment))\n",
        "for sentence in sent_segment:\n",
        "    print(sentence)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences in doc:  4\n",
            "We will discuss briefly with Dr. John about the basic syntax, structure and design philosophies.\n",
            "There is a hierarchical syntax for Python code which you should remember when writing code!\n",
            "Python is a really powerful programming language!\n",
            "Why is Python so special for you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxUcjDYp4vqm",
        "outputId": "844dd7cf-577e-4cce-c2bf-35fce9870b31"
      },
      "source": [
        "import nltk\n",
        "\n",
        "SENTENCE_TOKEN_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "\n",
        "# NLTK RegexSentenceTokenizer\n",
        "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKEN_PATTERN, gaps=True)\n",
        "\n",
        "text = \"We will discuss briefly with Dr. John about the basic syntax, \" \\\n",
        "\"structure and design philosophies. There is a hierarchical syntax for \" \\\n",
        "\"Python code which you should remember when writing code! Python is a really powerful programming language! \" \\\n",
        "\"Why is Python so special for you?\"\n",
        "\n",
        "sent_segment = regex_st.tokenize(text)\n",
        "print(\"Total sentences in doc: \", len(sent_segment))\n",
        "for sentence in sent_segment:\n",
        "    print(sentence)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences in doc:  4\n",
            "We will discuss briefly with Dr. John about the basic syntax, structure and design philosophies.\n",
            "There is a hierarchical syntax for Python code which you should remember when writing code!\n",
            "Python is a really powerful programming language!\n",
            "Why is Python so special for you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhxkVk0B4vqn"
      },
      "source": [
        "# Sentence Segmentation using Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAxuvm2M4vqn",
        "outputId": "f7117220-3411-48b0-f969-95e9595bc89a"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# spacy sentence tokenizer\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(\"We will discuss briefly with Dr. John about the basic syntax, \" \\\n",
        "\"structure and design philosophies. There is a hierarchical syntax \" \\\n",
        "\"for Python code which you should remember when writing code! Python is \" \\\n",
        "\"a really powerful programming language!\")\n",
        "\n",
        "#to print sentences \n",
        "for sent in doc.sents: \n",
        "    print(sent)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We will discuss briefly with Dr. John about the basic syntax, structure and design philosophies.\n",
            "There is a hierarchical syntax for Python code which you should remember when writing code!\n",
            "Python is a really powerful programming language!\n"
          ]
        }
      ]
    }
  ]
}