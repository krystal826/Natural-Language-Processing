{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "HMM-POS-tag.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krystal826/Natural-Language-Processing/blob/main/HMM_POS_tag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOvV1OIL9ZvL"
      },
      "source": [
        "# https://www.katrinerk.com/courses/python-worksheets/hidden-markov-models-for-pos-tagging-in-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x9vlL6D9ZvO"
      },
      "source": [
        "### This HMM addresses the problem of part-of-speech tagging. It estimates the probability of a tag sequence for a given word sequence as follows:\n",
        "### Say words = w1....wN and tags = t1..tN then\n",
        "### P(tags | words) is_proportional_to  product P(ti | t{i-1}) P(wi | ti)\n",
        "### To find the best tag sequence for a given sequence of words, we want to find the tag sequence that has the maximum P(tags | words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP6qdi4o9ZvP"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pPAo7yK9ZvQ"
      },
      "source": [
        "### Estimating P(wi | ti) from corpus data using Maximum Likelihood Estimation (MLE):\n",
        "### P(wi | ti) = count(wi, ti) / count(ti)\n",
        "### We add an artificial \"start\" tag at the beginning of each sentence, and an artificial \"end\" tag at the end of each sentence.\n",
        "### So we start out with the brown tagged sentences, add the two artificial tags, and then make one long list of all the tag/word pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35lz4P2S9ZvQ"
      },
      "source": [
        "brown_tags_words = [ ]\n",
        "for sent in brown.tagged_sents():\n",
        "    # sent is a list of word/tag pairs\n",
        "    # add START/START at the beginning\n",
        "    brown_tags_words.append( (\"START\", \"START\") )\n",
        "    # then all the tag/word pairs for the word/tag pairs in the sentence.\n",
        "    # shorten tags to 2 characters each\n",
        "    brown_tags_words.extend([ (tag[:2], word) for (word, tag) in sent ])\n",
        "    # then END/END\n",
        "    brown_tags_words.append( (\"END\", \"END\") )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5WGIYJe9ZvQ",
        "outputId": "4f251886-dc08-4270-8b9f-36f7d09df1a2"
      },
      "source": [
        "# conditional frequency distribution\n",
        "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words)\n",
        "\n",
        "# conditional probability distribution (EMISSION PROBABILITY)\n",
        "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
        "print(\"The probability of an adjective (JJ) being 'new' is\", cpd_tagwords[\"JJ\"].prob(\"new\"))\n",
        "print(\"The probability of a verb (VB) being 'duck' is\", cpd_tagwords[\"VB\"].prob(\"duck\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probability of an adjective (JJ) being 'new' is 0.01472344917632025\n",
            "The probability of a verb (VB) being 'duck' is 6.042713350943527e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88eBcBF_9ZvR"
      },
      "source": [
        "# Estimating P(ti | t{i-1}) from corpus data using Maximum Likelihood Estimation (MLE):\n",
        "\n",
        "# P(ti | t{i-1}) = count(t{i-1}, ti) / count(t{i-1})\n",
        "brown_tags = [tag for (tag, word) in brown_tags_words ]\n",
        "\n",
        "# make conditional frequency distribution:\n",
        "# count(t{i-1} ti)\n",
        "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt7f8UnE9ZvS",
        "outputId": "ec435aa3-4029-4bb2-c9dc-ae363bc0f7bd"
      },
      "source": [
        "# make conditional probability distribution (TRANSITION PROBABILITY), using MLE:\n",
        "\n",
        "# P(ti | t{i-1})\n",
        "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
        "\n",
        "print(\"If we have just seen 'DT', the probability of 'NN' is\", cpd_tags[\"DT\"].prob(\"NN\"))\n",
        "print( \"If we have just seen 'VB', the probability of 'JJ' is\", cpd_tags[\"VB\"].prob(\"DT\"))\n",
        "print( \"If we have just seen 'VB', the probability of 'NN' is\", cpd_tags[\"VB\"].prob(\"NN\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If we have just seen 'DT', the probability of 'NN' is 0.5057722522030194\n",
            "If we have just seen 'VB', the probability of 'JJ' is 0.016885067592065053\n",
            "If we have just seen 'VB', the probability of 'NN' is 0.10970977711020183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GkB-hdi9ZvT"
      },
      "source": [
        "### putting things together:\n",
        "### what is the probability of the tag sequence \"PP VB TO VB\" for the word sequence \"I want to race\"?\n",
        "### It is\n",
        "### P(START) * P(PP|START) * P(I | PP) *  P(VB | PP) * P(want | VB) *  P(TO | VB) * P(to | TO) *\n",
        "### P(VB | TO) * P(race | VB) * P(END | VB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ozNQFT89ZvT",
        "outputId": "ea4396b3-ad3d-4b17-b711-f19993d27d9d"
      },
      "source": [
        "# We leave aside P(START) for now.\n",
        "prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"I\") * \\\n",
        "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"want\") * \\\n",
        "    cpd_tags[\"VB\"].prob(\"TO\") * cpd_tagwords[\"TO\"].prob(\"to\") * \\\n",
        "    cpd_tags[\"TO\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"race\") * \\\n",
        "    cpd_tags[\"VB\"].prob(\"END\")\n",
        "print( \"The probability of the tag sequence 'START PP VB TO VB END' for 'I want to race' is:\", prob_tagsequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probability of the tag sequence 'START PP VB TO VB END' for 'I want to race' is: 1.0817766461150474e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIVeH_ne9ZvU",
        "outputId": "f2f1b5fe-de0a-4ac7-8674-56b47b695039"
      },
      "source": [
        "# We leave aside P(START) for now. I go to school\n",
        "prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"I\") * \\\n",
        "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"go\") * \\\n",
        "    cpd_tags[\"VB\"].prob(\"TO\") * cpd_tagwords[\"TO\"].prob(\"to\") * \\\n",
        "    cpd_tags[\"TO\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"school\") * \\\n",
        "    cpd_tags[\"VB\"].prob(\"END\")\n",
        "print( \"The probability of the tag sequence 'START PP VB TO VB END' for 'I go to school' is:\", prob_tagsequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probability of the tag sequence 'START PP VB TO VB END' for 'I go to school' is: 2.061176953481037e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiI_8N979ZvU",
        "outputId": "7786011e-1aa7-46aa-ed97-86bebc911c64"
      },
      "source": [
        "# We leave aside P(START) for now. I go to school\n",
        "prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"I\") * \\\n",
        "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"go\") * \\\n",
        "    cpd_tags[\"VB\"].prob(\"TO\") * cpd_tagwords[\"TO\"].prob(\"to\") * \\\n",
        "    cpd_tags[\"TO\"].prob(\"VB\") * cpd_tagwords[\"NN\"].prob(\"school\") * \\\n",
        "    cpd_tags[\"VB\"].prob(\"END\")\n",
        "print( \"The probability of the tag sequence 'START PP VB TO NN END' for 'I go to school' is:\", prob_tagsequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probability of the tag sequence 'START PP VB TO NN END' for 'I go to school' is: 1.0822300081041521e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZftERP49ZvV",
        "outputId": "98cbfaf5-affa-4826-d23a-f1eca9c36dc7"
      },
      "source": [
        "prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"I\") * \\\n",
        "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"saw\") * \\\n",
        "    cpd_tags[\"VB\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"her\") * \\\n",
        "    cpd_tags[\"PP\"].prob(\"NN\") * cpd_tagwords[\"NN\"].prob(\"duck\") * \\\n",
        "    cpd_tags[\"NN\"].prob(\"END\")\n",
        "print( \"The probability of the tag sequence 'START PP VB PP NN END' for 'I saw her duck' is:\", prob_tagsequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probability of the tag sequence 'START PP VB PP NN END' for 'I saw her duck' is: 3.372745049070759e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMov2ryW9ZvV",
        "outputId": "a8aa7d7a-92d9-4398-c27d-56e1f5862fa1"
      },
      "source": [
        "prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"I\") * \\\n",
        "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"saw\") * \\\n",
        "    cpd_tags[\"VB\"].prob(\"PP\") * cpd_tagwords[\"PP\"].prob(\"her\") * \\\n",
        "    cpd_tags[\"PP\"].prob(\"VB\") * cpd_tagwords[\"VB\"].prob(\"duck\") * \\\n",
        "    cpd_tags[\"VB\"].prob(\"END\")\n",
        "\n",
        "print( \"The probability of the tag sequence 'START PP VB PP VB END' for 'I saw her duck' is:\", prob_tagsequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The probability of the tag sequence 'START PP VB PP VB END' for 'I saw her duck' is: 7.285965712199413e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnKIrlqf9ZvV"
      },
      "source": [
        "#####\n",
        "# Viterbi:\n",
        "# If we have a word sequence, what is the best tag sequence?\n",
        "#\n",
        "# The method above lets us determine the probability for a single tag sequence.\n",
        "# But in order to find the best tag sequence, we need the probability\n",
        "# for _all_ tag sequence.\n",
        "# What Viterbi gives us is just a good way of computing all those many probabilities\n",
        "# as fast as possible.\n",
        "# what is the list of all tags?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC1QRSN89ZvW"
      },
      "source": [
        "distinct_tags = set(brown_tags)\n",
        "#sentence = [\"I\", \"want\", \"to\", \"race\" ]\n",
        "sentence = [\"I\", \"saw\", \"her\", \"duck\" ]\n",
        "sentlen = len(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc2Iy1Iw9ZvW"
      },
      "source": [
        "# viterbi:\n",
        "# for each step i in 1 .. sentlen,\n",
        "# store a dictionary that maps each tag X\n",
        "# to the probability of the best tag sequence of length i that ends in X\n",
        "\n",
        "viterbi = [ ]\n",
        "\n",
        "# backpointer:\n",
        "# for each step i in 1..sentlen,\n",
        "# store a dictionary that maps each tag X\n",
        "# to the previous tag in the best tag sequence of length i that ends in X\n",
        "\n",
        "backpointer = [ ]\n",
        "first_viterbi = { }\n",
        "first_backpointer = { }\n",
        "for tag in distinct_tags:\n",
        "    # don't record anything for the START tag\n",
        "    if tag == \"START\": continue\n",
        "    first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[0] )\n",
        "    first_backpointer[ tag ] = \"START\"\n",
        "\n",
        "#print(first_viterbi)\n",
        "#print(first_backpointer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjGBzE7E9ZvX",
        "outputId": "eaa14436-b9d2-4eb6-c1a4-7475e00f101e"
      },
      "source": [
        "viterbi.append(first_viterbi)\n",
        "backpointer.append(first_backpointer)\n",
        "currbest = max(first_viterbi.keys(), key = lambda tag: first_viterbi[ tag ])\n",
        "print( \"Word\", \"'\" + sentence[0] + \"'\", \"current best two-tag sequence:\", first_backpointer[ currbest], currbest)\n",
        "print( \"Word\", \"'\" + sentence[0] + \"'\", \"current best tag:\", currbest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word 'I' current best two-tag sequence: START PP\n",
            "Word 'I' current best tag: PP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sEummjK9ZvX",
        "outputId": "f0175d1f-1c32-41f6-e934-3f88d965cbb0"
      },
      "source": [
        "for wordindex in range(1, len(sentence)):\n",
        "    this_viterbi = { }\n",
        "    this_backpointer = { }\n",
        "    prev_viterbi = viterbi[-1]\n",
        "  \n",
        "    for tag in distinct_tags:\n",
        "        # don't record anything for the START tag\n",
        "        if tag == \"START\": continue\n",
        "        # if this tag is X and the current word is w, then\n",
        "        # find the previous tag Y such that\n",
        "        # the best tag sequence that ends in X\n",
        "        # actually ends in Y X\n",
        "        # that is, the Y that maximizes\n",
        "        # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n",
        "        # The following command has the same notation\n",
        "        # that you saw in the sorted() command.\n",
        "\n",
        "        best_previous = max(prev_viterbi.keys(),\n",
        "                            key = lambda prevtag: \\\n",
        "            prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex]))\n",
        "\n",
        "        # Instead, we can also use the following longer code:\n",
        "        # best_previous = None\n",
        "        # best_prob = 0.0\n",
        "        # for prevtag in distinct_tags:\n",
        "        #    prob = prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex])\n",
        "        #    if prob > best_prob:\n",
        "        #        best_previous= prevtag\n",
        "        #        best_prob = prob\n",
        "        #\n",
        "        this_viterbi[ tag ] = prev_viterbi[ best_previous] * \\\n",
        "            cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[ tag].prob(sentence[wordindex])\n",
        "        this_backpointer[ tag ] = best_previous\n",
        "    currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
        "    print( \"Word\", \"'\" + sentence[ wordindex] + \"'\", \"current best two-tag sequence:\", this_backpointer[ currbest], currbest)\n",
        "    print( \"Word\", \"'\" + sentence[ wordindex] + \"'\", \"current best tag:\", currbest)\n",
        "\n",
        "    # done with all tags in this iteration\n",
        "    # so store the current viterbi step\n",
        "    viterbi.append(this_viterbi)\n",
        "    backpointer.append(this_backpointer)\n",
        "# done with all words in the sentence.\n",
        "# now find the probability of each tag\n",
        "# to have \"END\" as the next tag,\n",
        "# and use that to find the overall best sequence\n",
        "\n",
        "prev_viterbi = viterbi[-1]\n",
        "best_previous = max(prev_viterbi.keys(),\n",
        "                    key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(\"END\"))\n",
        "\n",
        "prob_tagsequence = prev_viterbi[ best_previous ] * cpd_tags[ best_previous].prob(\"END\")\n",
        "# best tagsequence: we store this in reverse for now, will invert later\n",
        "best_tagsequence = [ \"END\", best_previous ]\n",
        "\n",
        "# invert the list of backpointers\n",
        "backpointer.reverse()\n",
        "# go backwards through the list of backpointers\n",
        "# (or in this case forward, because we have inverter the backpointer list)\n",
        "# in each case:\n",
        "# the following best tag is the one listed under\n",
        "# the backpointer for the current best tag\n",
        "\n",
        "current_best_tag = best_previous\n",
        "for bp in backpointer:\n",
        "    best_tagsequence.append(bp[current_best_tag])\n",
        "    current_best_tag = bp[current_best_tag]\n",
        "best_tagsequence.reverse()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word 'saw' current best two-tag sequence: PP VB\n",
            "Word 'saw' current best tag: VB\n",
            "Word 'her' current best two-tag sequence: VB PP\n",
            "Word 'her' current best tag: PP\n",
            "Word 'duck' current best two-tag sequence: PP VB\n",
            "Word 'duck' current best tag: VB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBYk-9Y69ZvX",
        "outputId": "a2b2e176-094b-4ace-f0ed-992e8547dc0f"
      },
      "source": [
        "print( \"The sentence was:\", end = \" \")\n",
        "for w in sentence: print( w, end = \" \")\n",
        "print(\"\\n\")\n",
        "print( \"The best tag sequence is:\", end = \" \")\n",
        "for t in best_tagsequence: print (t, end = \" \")\n",
        "print(\"\\n\")\n",
        "print( \"The probability of the best tag sequence is:\", prob_tagsequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sentence was: I saw her duck \n",
            "\n",
            "The best tag sequence is: START PP VB PP VB END \n",
            "\n",
            "The probability of the best tag sequence is: 7.285965712199413e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAfpniTl9ZvY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}